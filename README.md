# Configuration Automatique des R√©servations Kubelet

> Script bash pour configurer dynamiquement `system-reserved` et `kube-reserved` sur des n≈ìuds Kubernetes v1.32+

[![Kubernetes](https://img.shields.io/badge/kubernetes-v1.32-blue.svg)](https://kubernetes.io/)
[![Bash](https://img.shields.io/badge/bash-5.0+-green.svg)](https://www.gnu.org/software/bash/)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)

---

## üìã Table des mati√®res

- [Vue d'ensemble](#-vue-densemble)
- [Pr√©requis](#-pr√©requis)
- [Installation](#-installation)
- [Utilisation](#-utilisation)
- [Profils disponibles](#-profils-disponibles)
- [Density-factor](#%EF%B8%8F-density-factor--adapter-selon-la-densit√©-de-pods)
- [Exemples d'utilisation](#-exemples-dutilisation)
- [D√©ploiement sur un cluster](#-d√©ploiement-sur-un-cluster)
- [Validation post-d√©ploiement](#-validation-post-d√©ploiement)
- [Rollback](#-rollback)
- [FAQ](#-faq)
- [Troubleshooting](#-troubleshooting)
- [Monitoring et m√©triques](#-monitoring-et-m√©triques)
- [S√©curit√© et bonnes pratiques](#-s√©curit√©-et-bonnes-pratiques)
- [Ressources suppl√©mentaires](#-ressources-suppl√©mentaires)
- [Contribution](#-contribution)
- [Changelog](#-changelog-et-notes-de-version)
- [Licence](#-licence)
- [Support](#-support)
- [Cr√©dits](#-cr√©dits)

---

## üéØ Vue d'ensemble

Ce script automatise la configuration des r√©servations de ressources kubelet (`system-reserved` et `kube-reserved`) en :

- ‚úÖ **D√©tectant automatiquement** les ressources du n≈ìud (vCPU, RAM)
- ‚úÖ **Calculant les r√©servations optimales** selon plusieurs profils (GKE, EKS, Conservative, Minimal)
- ‚úÖ **Adaptant dynamiquement** selon la densit√© de pods cible (via `density-factor`)
- ‚úÖ **G√©n√©rant la configuration kubelet** compl√®te
- ‚úÖ **Appliquant et validant** la configuration

### Pourquoi ce script ?

Les r√©servations `system-reserved` et `kube-reserved` sont **critiques** pour la stabilit√© des n≈ìuds Kubernetes :
- **Sous-dimensionn√©es** ‚Üí OOM kills, √©victions, node NotReady
- **Sur-dimensionn√©es** ‚Üí Gaspillage de capacit√© allocatable

Ce script applique les **formules officielles** document√©es par Google (GKE), Amazon (EKS) et Red Hat (OpenShift).

---

## üîß Pr√©requis

### Syst√®me d'exploitation
- Ubuntu 20.04+ avec systemd
- Noyau Linux 5.x+ (pour cgroups v2, recommand√©)

### Kubernetes
- Version **v1.26+** (test√© sur v1.32)
- Container runtime : **containerd** (recommand√©) ou CRI-O
- Kubelet configur√© avec `cgroupDriver: systemd`

### D√©pendances

**‚ú® Installation automatique** : Le script installe automatiquement les d√©pendances manquantes (bc, jq, yq v4) au premier lancement. Aucune action pr√©alable requise !

Le script n√©cessite les outils suivants :
- `bc` : Calculs arithm√©tiques
- `jq` : Traitement JSON
- `yq` v4+ (mikefarah) : Traitement YAML

**Installation automatique** :
```bash
# Les d√©pendances sont install√©es automatiquement lors de l'ex√©cution
sudo ./kubelet_auto_config.sh --dry-run
# Le script d√©tecte et installe bc, jq, et yq v4 si n√©cessaire
```

**Installation manuelle** (optionnelle) :
```bash
sudo apt update
sudo apt install -y bc jq

# Installer yq (>= v4) - le paquet Ubuntu (v3) n'est pas compatible
ARCH=$(uname -m)
case "$ARCH" in
  x86_64|amd64)   YQ_BIN=yq_linux_amd64 ;;
  arm64|aarch64)  YQ_BIN=yq_linux_arm64 ;;
  *) echo "Architecture non support√©e: $ARCH" >&2; exit 1 ;;
esac

sudo wget -qO /usr/local/bin/yq "https://github.com/mikefarah/yq/releases/download/v4.44.3/${YQ_BIN}"
sudo chmod +x /usr/local/bin/yq

# V√©rifier la version
yq --version
```

> ‚ÑπÔ∏è **Important** : le script installe automatiquement `yq` **v4+** (binaire mikefarah).
> Si `yq` Python v3 est d√©j√† install√©, le script le remplace automatiquement par la bonne version.
> Le paquet `apt install yq` installe la version Python 3.x qui provoque des erreurs `yq: -i/--in-place can only be used with -y/-Y`.
> Consultez les binaires officiels : https://github.com/mikefarah/yq/releases

### Permissions

Le script doit √™tre ex√©cut√© en tant que **root** ou avec **sudo** :
```bash
sudo ./kubelet_auto_config.sh
```

---

## üì¶ Installation

### M√©thode 1 : T√©l√©chargement direct

```bash
# T√©l√©charger le script
curl -O https://gitlab.com/omega8280051/reserved-sys-kube/-/raw/main/kubelet_auto_config.sh

# Rendre ex√©cutable
chmod +x kubelet_auto_config.sh

# V√©rifier les d√©pendances
./kubelet_auto_config.sh --help
```

### M√©thode 2 : Via Git

```bash
git clone https://gitlab.com/omega8280051/reserved-sys-kube.git
cd reserved-sys-kube
chmod +x kubelet_auto_config.sh
```

### M√©thode 3 : D√©ploiement sur tous les n≈ìuds

```bash
# Copier le script sur tous les n≈ìuds via SSH
NODES="node1 node2 node3"  # Remplacer par vos n≈ìuds
for node in $NODES; do
    scp kubelet_auto_config.sh root@$node:/usr/local/bin/
    ssh root@$node "chmod +x /usr/local/bin/kubelet_auto_config.sh"
done
```

---

## üöÄ Utilisation

### Syntaxe g√©n√©rale

```bash
sudo ./kubelet_auto_config.sh [OPTIONS]
```

### Options disponibles

| Option | Description | Valeur par d√©faut |
|--------|-------------|-------------------|
| `--profile <profil>` | Profil de calcul : `gke`, `eks`, `conservative`, `minimal` | `gke` |
| `--density-factor <float>` | Multiplicateur pour haute densit√© (0.1 √† 5.0, recommand√© 0.5-3.0) | `1.0` |
| `--target-pods <int>` | Nombre de pods cible (calcul auto du density-factor) | - |
| `--node-type <type>` | Type de n≈ìud : `control-plane`, `worker`, `auto` (d√©tection auto) | `auto` |
| `--dry-run` | Affiche la configuration sans l'appliquer | `false` |
| `--backup` | Cr√©e un backup permanent timestamp√© (en plus des 4 backups rotatifs automatiques) | `false` |
| `--help` | Affiche l'aide | - |

### Workflow recommand√©

```
1. Test en dry-run        ‚Üí V√©rifier les valeurs calcul√©es
2. Backup                 ‚Üí Sauvegarder la config existante
3. Application            ‚Üí Appliquer la nouvelle config
4. Validation             ‚Üí V√©rifier allocatable et stabilit√©
```

---

## üìö Profils disponibles

### 1. **GKE** (Google Kubernetes Engine) - Recommand√©

**Cas d'usage** : Clusters production g√©n√©ralistes

**Caract√©ristiques** :
- Formules officielles Google Cloud
- √âquilibre stabilit√© / capacit√©
- Test√© √† grande √©chelle (>100k n≈ìuds)

**Exemple** :
```bash
sudo ./kubelet_auto_config.sh --profile gke
```

**R√©sultat typique (16 vCPU / 64 GiB)** :
- system-reserved : `300m CPU, 1123Mi RAM`
- kube-reserved : `220m CPU, 959Mi RAM`
- Allocatable : `15480m CPU, 62.08 GiB RAM`

---

### 2. **EKS** (Amazon Elastic Kubernetes Service)

**Cas d'usage** : Clusters AWS, compatibilit√© EKS

**Caract√©ristiques** :
- Formules officielles Amazon EKS
- R√©servations par paliers (< 8 vCPU, 8-32 vCPU, > 32 vCPU)

**Exemple** :
```bash
sudo ./kubelet_auto_config.sh --profile eks
```

---

### 3. **Conservative** (OpenShift-like)

**Cas d'usage** : Environnements critiques, workloads sensibles

**Caract√©ristiques** :
- Inspir√© de Red Hat OpenShift
- R√©servations major√©es (+30-50% vs GKE)
- Privil√©gie la stabilit√© sur la capacit√©

**Exemple** :
```bash
sudo ./kubelet_auto_config.sh --profile conservative
```

**R√©sultat typique (16 vCPU / 64 GiB)** :
- system-reserved : `660m CPU, 2355Mi RAM`
- kube-reserved : `740m CPU, 4301Mi RAM`
- Allocatable : `14600m CPU, 57.52 GiB RAM`

---

### 4. **Minimal**

**Cas d'usage** : Environnements dev/test, maximiser allocatable

**Caract√©ristiques** :
- R√©servations minimales
- ‚ö†Ô∏è **Attention** : Monitoring requis, risque instabilit√©

**Exemple** :
```bash
sudo ./kubelet_auto_config.sh --profile minimal --dry-run
```

---

## üñ•Ô∏è D√©tection automatique Control-Plane vs Worker

### Pourquoi cette distinction ?

Le script d√©tecte automatiquement le type de n≈ìud et adapte la configuration `enforceNodeAllocatable` :

| Type | enforceNodeAllocatable | Raison |
|------|------------------------|--------|
| **Control-plane** | `["pods", "system-reserved"]` | Les static pods critiques (kube-apiserver, etcd, etc.) doivent d√©marrer **avant** le kubelet. Si `kube-reserved` est enforced, ces pods ne peuvent pas d√©marrer ‚Üí cluster cass√©. |
| **Worker** | `["pods", "system-reserved", "kube-reserved"]` | Enforcement complet recommand√© pour maximiser la stabilit√©. |

### D√©tection automatique (par d√©faut)

Le script d√©tecte automatiquement le type en v√©rifiant la pr√©sence de static pods dans `/etc/kubernetes/manifests/` :

```bash
# Ex√©cution normale (d√©tection auto)
sudo ./kubelet_auto_config.sh

# Sortie sur un control-plane :
# [INFO] D√©tection du type de n≈ìud...
# [SUCCESS] N≈ìud d√©tect√©: CONTROL-PLANE (static pods d√©tect√©s dans /etc/kubernetes/manifests)
# [WARNING] Mode control-plane: kube-reserved ne sera PAS enforced (pour pr√©server les static pods critiques)

# Sortie sur un worker :
# [INFO] D√©tection du type de n≈ìud...
# [SUCCESS] N≈ìud d√©tect√©: WORKER (aucun static pod control-plane trouv√©)
# [INFO] Mode worker: kube-reserved sera enforced normalement
```

### Override manuel (si n√©cessaire)

Dans de rares cas, vous pouvez forcer le type manuellement :

```bash
# Forcer mode control-plane
sudo ./kubelet_auto_config.sh --node-type control-plane

# Forcer mode worker
sudo ./kubelet_auto_config.sh --node-type worker

# Mode auto (par d√©faut, peut √™tre omis)
sudo ./kubelet_auto_config.sh --node-type auto
```

### Important : Control-planes avec taints

Si vos control-planes ont des **taints** et n'ex√©cutent jamais de workloads utilisateur, vous pouvez r√©duire les r√©servations :

```bash
# Control-plane d√©di√© (pas de workloads)
sudo ./kubelet_auto_config.sh --profile minimal --node-type control-plane

# Control-plane mixte (avec workloads)
sudo ./kubelet_auto_config.sh --profile gke --node-type control-plane
```

---

## üéõÔ∏è Density-factor : Adapter selon la densit√© de pods

### Concept

Le **density-factor** est un multiplicateur appliqu√© aux r√©servations pour **adapter les ressources kubelet selon la charge**.

**Pourquoi ?** Plus il y a de pods sur un n≈ìud, plus kubelet consomme de ressources :

- **R√©conciliation** : Kubelet v√©rifie l'√©tat de chaque pod toutes les 10 secondes (PLEG - Pod Lifecycle Event Generator)
- **Health checks** : Ex√©cution des liveness/readiness probes de tous les pods
- **API calls** : Communication avec l'API server et le container runtime (containerd/CRI-O)
- **Logging & metrics** : Collecte de logs et m√©triques de tous les containers
- **Watch operations** : Surveillance des changements d'√©tat des pods, secrets, configmaps

**R√®gle simple** :
- **30 pods** ‚Üí density-factor **1.0** (baseline, r√©servations de base)
- **80 pods** ‚Üí density-factor **1.2** (+20% de ressources pour kubelet)
- **110 pods** ‚Üí density-factor **1.5** (+50% de ressources pour kubelet)

üí° **Le script calcule automatiquement** le bon facteur avec `--target-pods` !

### Tableau de recommandations

| Pods/n≈ìud | Density-factor | Commande |
|-----------|----------------|----------|
| **0-30** (faible) | `1.0` (baseline) | Par d√©faut |
| **31-50** (standard) | `1.1` (+10%) | `--density-factor 1.1` |
| **51-80** (√©lev√©e) | `1.2` (+20%) | `--density-factor 1.2` |
| **81-110** (tr√®s √©lev√©e) | `1.5` (+50%) | `--density-factor 1.5` ou `--target-pods 110`* |
| **>110** (extr√™me) | `2.0` (+100%) | `--density-factor 2.0` + augmenter `maxPods` |

> ‚ö†Ô∏è *Sur des n≈ìuds modestes (‚â§‚ÄØ2 vCPU / ‚â§‚ÄØ2‚ÄØGiB), le script refusera ces facteurs √©lev√©s :
> il arr√™te l‚Äôex√©cution si l‚Äôallocatable projet√© descend sous 25‚ÄØ% CPU ou 20‚ÄØ% RAM (30‚ÄØ%/25‚ÄØ%
> pour un control-plane). Pr√©voyez des machines plus capacitaires avant de pousser la densit√©.*

### Calcul automatique

Le script peut **calculer automatiquement** le density-factor :

```bash
# Cluster avec 110 pods/n≈ìud maximum
sudo ./kubelet_auto_config.sh --profile conservative --target-pods 110

# Le script calcule automatiquement : density-factor = 1.5
```

### Exemples concrets

#### Cluster avec 20 pods/n≈ìud (faible densit√©)
```bash
# Pas de facteur n√©cessaire
sudo ./kubelet_auto_config.sh --profile gke
```

#### Cluster avec 80 pods/n≈ìud (haute densit√©)
```bash
# Facteur 1.2 recommand√©
sudo ./kubelet_auto_config.sh --profile conservative --density-factor 1.2
# Sur un control-plane, le facteur est plafonn√© √† 1.0 : un warning signalera l'ajustement.
# Sur un worker avec peu de m√©moire, le script peut refuser cette configuration.
```

#### Cluster avec 110 pods/n≈ìud (limite maximale)
```bash
# Calcul automatique du facteur
sudo ./kubelet_auto_config.sh --profile conservative --target-pods 110 --backup
# Si les ressources sont insuffisantes, le script refusera (allocatable < seuil minimal).

# Ou manuellement
sudo ./kubelet_auto_config.sh --profile conservative --density-factor 1.5 --backup
# Sur control-plane, le facteur sera ramen√© automatiquement √† 1.0.
# Sur petit worker, la commande peut √™tre bloqu√©e car les r√©servations d√©passent la capacit√©.
```

---

## üìñ Exemples d'utilisation

### Exemple 1 : Premier test (dry-run)

```bash
# Voir la configuration qui serait appliqu√©e, sans toucher au syst√®me
sudo ./kubelet_auto_config.sh --dry-run

# Sortie :
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#   CONFIGURATION KUBELET - R√âSERVATIONS CALCUL√âES
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# 
# Configuration n≈ìud:
#   vCPU:              16
#   RAM:               64 GiB
#   Profil:            gke
#   Density-factor:    1.0
# [...]
```

### Exemple 2 : Configuration standard production

```bash
# N≈ìud g√©n√©raliste, 30-50 pods maximum
sudo ./kubelet_auto_config.sh --profile gke --backup

# V√©rifier les logs kubelet
sudo journalctl -u kubelet -f
```

### Exemple 3 : Cluster haute densit√© (110 pods/n≈ìud)

```bash
# √âtape 1 : Dry-run pour v√©rifier
sudo ./kubelet_auto_config.sh \
  --profile conservative \
  --target-pods 110 \
  --dry-run

# √âtape 2 : Application avec backup
sudo ./kubelet_auto_config.sh \
  --profile conservative \
  --target-pods 110 \
  --backup

# √âtape 3 : Validation
kubectl describe node $(hostname) | grep -A 10 Allocatable
```

### Exemple 4 : Configuration personnalis√©e

```bash
# Profil conservative avec facteur custom
sudo ./kubelet_auto_config.sh \
  --profile conservative \
  --density-factor 1.3 \
  --backup
```

### Exemple 5 : Environnement dev/test (minimal)

```bash
# Maximiser la capacit√© allocatable (avec pr√©caution)
sudo ./kubelet_auto_config.sh \
  --profile minimal \
  --dry-run  # Toujours tester d'abord !
```

---

## üåê D√©ploiement sur un cluster

### Architecture cible

**Exemple** : Cluster de 220 n≈ìuds avec 110 pods/n≈ìud
- **Total** : 24,200 pods dans le cluster
- **Profil** : Conservative + density-factor 1.5

### Strat√©gie de d√©ploiement progressive

```
Phase 1 : 1 n≈ìud pilote     ‚Üí Validation 24-48h
Phase 2 : 10% (22 n≈ìuds)    ‚Üí Validation 7 jours
Phase 3 : 50% (110 n≈ìuds)   ‚Üí Validation 7 jours
Phase 4 : 100% (220 n≈ìuds)  ‚Üí Rollout complet
```

---

### M√©thode 1 : D√©ploiement manuel (petit cluster)

> üí° **Pr√©-requis** : acc√®s SSH sans mot de passe (cl√©) et paquets `bc`, `jq`, `yq >= 4` d√©j√† install√©s
> sur chaque n≈ìud. Adaptez `SSH_USER`/`SSH_OPTS` √† votre contexte (`root`, `vagrant`, `ec2-user`, etc.).

```bash
#!/bin/bash
# deploy-manual.sh

SSH_USER="root"
SSH_OPTS=""  # Exemple : "-F ~/.ssh/config" ou "-o StrictHostKeyChecking=no"
NODES="node1 node2"  # Liste de vos n≈ìuds
PROFILE="conservative"
TARGET_PODS="110"

for node in $NODES; do
    echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
    echo "Configuration du n≈ìud : $node"
    echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
    
    # Copier le script
    scp $SSH_OPTS kubelet_auto_config.sh ${SSH_USER}@$node:/tmp/
    
    # Ex√©cuter
    ssh $SSH_OPTS ${SSH_USER}@$node "chmod +x /tmp/kubelet_auto_config.sh && \
                    /tmp/kubelet_auto_config.sh \
                    --profile $PROFILE \
                    --target-pods $TARGET_PODS \
                    --backup"
    
    # V√©rifier le status
    ssh $SSH_OPTS ${SSH_USER}@$node "systemctl is-active kubelet"
    
    echo ""
    echo "‚úì N≈ìud $node configur√©"
    echo ""
    sleep 5
done

echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
echo "D√©ploiement termin√© sur tous les n≈ìuds"
echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
```

**Ex√©cution** :
```bash
chmod +x deploy-manual.sh
./deploy-manual.sh

> ‚ÑπÔ∏è Sur des n≈ìuds de faible capacit√© (‚â§‚ÄØ2‚ÄØvCPU / ‚â§‚ÄØ2‚ÄØGiB RAM), `--target-pods 110` peut √™tre refus√©
> car l‚Äôallocatable tomberait sous les seuils de s√©curit√©. Commencez par le profil `gke` ou r√©duisez
> le `density-factor`.
```

---

### M√©thode 2 : D√©ploiement via Ansible (recommand√©)

> ‚úÖ **Valid√© sur lab Vagrant** : Cette m√©thode a √©t√© test√©e et valid√©e (voir [ansible/README.md](ansible/README.md))

**Fichier** : `ansible/deploy-kubelet-config.yml` (version simplifi√©e)

```yaml
---
- name: Configuration des r√©servations kubelet
  hosts: k8s_workers
  become: yes
  vars:
    profile: "gke"
    target_pods: 110
    backup_enabled: true

  tasks:
    - name: V√©rifier d√©pendances (bc, jq)
      package:
        name: [bc, jq]
        state: present

    - name: Installer yq si n√©cessaire
      # Installation automatique de yq v4
      # (voir playbook complet pour les d√©tails)

    - name: Copier le script
      copy:
        src: kubelet_auto_config.sh
        dest: /usr/local/bin/kubelet_auto_config.sh
        mode: '0755'

    - name: Dry-run
      command: >
        /usr/local/bin/kubelet_auto_config.sh
        --profile {{ profile }}
        --target-pods {{ target_pods }}
        --dry-run
      failed_when: false

    - name: Appliquer configuration
      command: >
        /usr/local/bin/kubelet_auto_config.sh
        --profile {{ profile }}
        --target-pods {{ target_pods }}
        --backup

    - name: V√©rifier kubelet
      systemd:
        name: kubelet
        state: started
```

**Inventory** : `ansible/inventory.ini`

```ini
[k8s_workers]
node1.example.com
node2.example.com

[k8s_workers:vars]
ansible_user=root
ansible_ssh_private_key_file=~/.ssh/id_rsa
```

**Ex√©cution** :

```bash
# Installer Ansible
sudo apt update && sudo apt install -y ansible

# Test de connectivit√©
cd ansible
ansible -i inventory.ini all -m ping

# D√©ploiement
ansible-playbook -i inventory.ini deploy-kubelet-config.yml

# Personnalisation
ansible-playbook -i inventory.ini deploy-kubelet-config.yml \
  -e "profile=conservative" \
  -e "target_pods=80"
```

**R√©sultats valid√©s** (lab Vagrant, profil `gke`) :

```
PLAY RECAP:
  cp1  : ok=14   changed=4    failed=0
  w1   : ok=13   changed=4    failed=0

Allocatable:
  - cp1: 2670m CPU, 2.96 GiB RAM
  - w1:  1700m CPU, 1.08 GiB RAM
```

> ‚ö†Ô∏è **Garde-fous** : Sur des n≈ìuds contraints (‚â§ 2 vCPU / ‚â§ 2 GiB), les configurations agressives seront refus√©es (allocatable < 25% CPU / 20% RAM).

üìñ **Documentation compl√®te** : Voir **[ansible/README.md](ansible/README.md)** pour :
- Playbook complet avec installation automatique de yq
- Configuration inventory (depuis poste ou n≈ìud cluster)
- Guide troubleshooting d√©taill√©
- Exemples d'ex√©cution complets

---

### M√©thode 3 : D√©ploiement via DaemonSet (avanc√©)

> ‚úÖ **Valid√© sur lab Vagrant** : Cette m√©thode a √©t√© test√©e avec succ√®s (voir [daemonset/README.md](daemonset/README.md))

‚ö†Ô∏è **Attention** : Cette m√©thode n√©cessite des privil√®ges √©lev√©s (`privileged: true`, `hostPath`)
- √Ä utiliser uniquement dans des environnements contr√¥l√©s
- Validation s√©curit√© requise avant usage en production
- Non adapt√©e aux n≈ìuds tr√®s contraints (< 25% CPU / < 20% RAM apr√®s config)

**Script de d√©ploiement** : `daemonset/generate-daemonset.sh`

```bash
#!/bin/bash
# D√©ploiement automatique du DaemonSet

cd daemonset

# 1. Cr√©er le ConfigMap avec le script
kubectl create configmap kubelet-config-script \
  --from-file=kubelet_auto_config.sh=../kubelet_auto_config.sh \
  --namespace=kube-system \
  --dry-run=client -o yaml | kubectl apply -f -

# 2. D√©ployer le DaemonSet
kubectl apply -f kubelet-config-daemonset-only.yaml

# 3. V√©rifier les pods
kubectl get pods -n kube-system -l app=kubelet-config-updater -o wide
```

**Surveillance** :

```bash
# Suivre les logs de tous les pods
kubectl logs -n kube-system -l app=kubelet-config-updater -f

# Logs d'un pod sp√©cifique
kubectl logs -n kube-system kubelet-config-updater-xxxxx

# Si kubectl logs ne fonctionne pas, utiliser crictl sur le n≈ìud
ssh node1 "sudo crictl logs <container-id>"
```

**V√©rification** :

```bash
# V√©rifier l'allocatable apr√®s application
kubectl get nodes -o custom-columns=\
NAME:.metadata.name,\
CPU-ALLOC:.status.allocatable.cpu,\
MEM-ALLOC:.status.allocatable.memory
```

**Nettoyage** :

```bash
# Supprimer le DaemonSet apr√®s application
kubectl delete daemonset -n kube-system kubelet-config-updater
kubectl delete configmap -n kube-system kubelet-config-script
```

**Personnalisation** : √âditez `daemonset/kubelet-config-daemonset-only.yaml` pour modifier le profil ou target-pods :

```yaml
chroot /host /tmp/kubelet_auto_config.sh \
  --profile gke \            # ou eks, conservative, minimal
  --target-pods 80 \         # ajuster selon votre densit√©
  --backup
```

üìñ **Documentation compl√®te** : Voir **[daemonset/README.md](daemonset/README.md)** pour :
- Guide de d√©ploiement d√©taill√© (automatique et manuel)
- Surveillance avec kubectl et crictl
- Troubleshooting complet
- Comparaison avec les M√©thodes 1 & 2
- R√©sultats de validation sur lab r√©el
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: kubelet-config-script
  namespace: kube-system
data:
  kubelet_auto_config.sh: |
    # Coller ici le contenu du script bash
```

**D√©ploiement** :
```bash
kubectl apply -f kubelet-config-daemonset.yaml

# Suivre les logs
kubectl logs -n kube-system -l app=kubelet-config-updater -f

# Supprimer apr√®s d√©ploiement
kubectl delete daemonset -n kube-system kubelet-config-updater
```

---

## ‚úÖ Validation post-d√©ploiement

### 1. V√©rifier l'allocatable sur tous les n≈ìuds

```bash
kubectl get nodes -o custom-columns=\
NAME:.metadata.name,\
CPU-CAP:.status.capacity.cpu,\
CPU-ALLOC:.status.allocatable.cpu,\
MEM-CAP:.status.capacity.memory,\
MEM-ALLOC:.status.allocatable.memory
```

**Sortie attendue** :
```
NAME      CPU-CAP   CPU-ALLOC   MEM-CAP      MEM-ALLOC
node1     16        15480m      67108864Ki   63572992Ki
node2     16        15480m      67108864Ki   63572992Ki
node3     16        15480m      67108864Ki   63572992Ki
```

### 2. V√©rifier qu'aucun n≈ìud n'est NotReady

```bash
kubectl get nodes

# Tous les n≈ìuds doivent √™tre Ready
# Si un n≈ìud est NotReady, v√©rifier les logs :
kubectl describe node <node-name>
```

### 3. V√©rifier les cgroups sur un n≈ìud

```bash
# Se connecter √† un n≈ìud
ssh node1

# V√©rifier la hi√©rarchie cgroups
systemd-cgls | grep -E "(system.slice|kubepods.slice|kubelet.slice)"

# Sortie attendue :
# ‚îú‚îÄsystem.slice
# ‚îÇ ‚îú‚îÄcontainerd.service
# ‚îÇ ‚îî‚îÄ...
# ‚îú‚îÄkubelet.slice
# ‚îÇ ‚îî‚îÄkubelet.service
# ‚îî‚îÄkubepods.slice
#   ‚îú‚îÄkubepods-burstable.slice
#   ‚îî‚îÄkubepods-besteffort.slice
```

### 4. V√©rifier les m√©triques kubelet

```bash
# Sur un n≈ìud
curl -s http://localhost:10255/metrics | grep -E "(kubelet_runtime_operations|kubelet_pleg)"

# M√©triques cl√©s :
# - kubelet_pleg_relist_duration_seconds : doit √™tre < 5s
# - kubelet_runtime_operations_duration_seconds : doit √™tre < 2s
```

### 5. V√©rifier les √©victions

```bash
# Aucune √©viction due √† pression m√©moire ne devrait appara√Ætre
kubectl get events --all-namespaces --field-selector reason=Evicted

# Si des √©victions apparaissent, augmenter les r√©servations
```

### 6. Test de charge (optionnel)

```bash
# D√©ployer un workload de test
kubectl create deployment stress-test --image=polinux/stress \
  --replicas=10 -- stress --cpu 1 --vm 1 --vm-bytes 512M --timeout 300s

# Observer la stabilit√© des n≈ìuds
watch -n 2 "kubectl top nodes"

# Nettoyer
kubectl delete deployment stress-test
```

---

## üîÑ Rollback

### En cas de probl√®me

Le script v2.0.3+ conserve automatiquement **plusieurs niveaux de backups** pour faciliter les rollbacks.

#### 1. Restaurer depuis les backups rotatifs (automatiques)

```bash
# Le script conserve automatiquement les 4 derni√®res configurations r√©ussies
# Format : /var/lib/kubelet/config.yaml.last-success.{0,1,2,3}
# .0 = configuration actuelle, .1 = pr√©c√©dente, etc.

# Lister les backups rotatifs disponibles
ls -lht /var/lib/kubelet/config.yaml.last-success.*

# Revenir √† la derni√®re configuration stable (1 changement en arri√®re)
sudo cp /var/lib/kubelet/config.yaml.last-success.1 /var/lib/kubelet/config.yaml
sudo systemctl restart kubelet

# Revenir 2 changements en arri√®re
sudo cp /var/lib/kubelet/config.yaml.last-success.2 /var/lib/kubelet/config.yaml
sudo systemctl restart kubelet

# Revenir 3 changements en arri√®re (si disponible)
sudo cp /var/lib/kubelet/config.yaml.last-success.3 /var/lib/kubelet/config.yaml
sudo systemctl restart kubelet
```

#### 2. Restaurer depuis les backups permanents (--backup)

```bash
# Si vous avez utilis√© --backup, des backups permanents timestamp√©s sont conserv√©s
# Format : /var/lib/kubelet/config.yaml.backup.YYYYMMDD_HHMMSS

# Lister les backups permanents
ls -lh /var/lib/kubelet/config.yaml.backup.*

# Restaurer un backup permanent sp√©cifique
sudo cp /var/lib/kubelet/config.yaml.backup.20251021_101234 /var/lib/kubelet/config.yaml
sudo systemctl restart kubelet

# Ou restaurer le dernier backup permanent
LATEST_BACKUP=$(ls -t /var/lib/kubelet/config.yaml.backup.* 2>/dev/null | head -1)
sudo cp "$LATEST_BACKUP" /var/lib/kubelet/config.yaml
sudo systemctl restart kubelet
```

#### 3. Script de rollback automatique

```bash
# Script fourni dans le d√©p√¥t : rollback-kubelet-config.sh

# Restaurer automatiquement la configuration pr√©c√©dente
sudo ./rollback-kubelet-config.sh

# Voir le backup s√©lectionn√© sans appliquer
sudo ./rollback-kubelet-config.sh --dry-run

# Restaurer un backup pr√©cis (.last-success.2)
sudo ./rollback-kubelet-config.sh --index 2

# Restaurer sans red√©marrer kubelet (√† faire manuellement ensuite)
sudo ./rollback-kubelet-config.sh --no-restart
```

Le script :
- ignore `.last-success.0` (configuration actuelle) et restaure par d√©faut `.1` ;
- bascule automatiquement vers les backups permanents (`--backup`) si besoin ;
- expose des options `--index`, `--dry-run`, `--no-restart`.

#### 4. Configuration manuelle d'urgence

Si le kubelet ne d√©marre plus :

```bash
# √âditer manuellement la config
sudo vi /var/lib/kubelet/config.yaml

# Supprimer ou ajuster les sections systemReserved et kubeReserved
# Exemple minimal qui fonctionne toujours :
# systemReserved:
#   cpu: "100m"
#   memory: "512Mi"
# kubeReserved:
#   cpu: "100m"
#   memory: "512Mi"

# Red√©marrer
sudo systemctl restart kubelet
sudo journalctl -u kubelet -f
```

---

## ‚ùì FAQ

### Q1 : Le script modifie-t-il d'autres param√®tres kubelet ?

**R** : Non, le script pr√©serve **intelligemment** vos configurations existantes (depuis v2.0.5).

**Comportement :**
- ‚úÖ **Si `/var/lib/kubelet/config.yaml` existe** : Le script **fusionne** avec la configuration existante
  - Modifie uniquement : `systemReserved`, `kubeReserved`, `enforceNodeAllocatable`, cgroups, seuils d'√©viction
  - **Pr√©serve tous les autres param√®tres** : `maxPods`, `imageGCHighThresholdPercent`, `rotateCertificates`, etc.
  - Vos tweaks personnalis√©s sont **conserv√©s** !

- ‚úÖ **Si aucune configuration n'existe** : Le script g√©n√®re une configuration compl√®te avec les valeurs par d√©faut Kubernetes

**Exemple :**
```yaml
# Configuration existante avec tweaks
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
maxPods: 150                          # ‚Üê Tweak personnalis√©
imageGCHighThresholdPercent: 90       # ‚Üê Tweak personnalis√©
systemReserved:
  cpu: "100m"                         # ‚Üê Sera mis √† jour par le script
  memory: "512Mi"                     # ‚Üê Sera mis √† jour par le script

# Apr√®s ex√©cution du script
# maxPods et imageGCHighThresholdPercent restent inchang√©s
# Seuls systemReserved et kubeReserved sont mis √† jour
```

### Q2 : Puis-je ex√©cuter le script plusieurs fois ?

**R** : Oui, le script est **idempotent**. Vous pouvez le relancer sans risque.

**Gestion automatique des backups** (depuis v2.0.3) :
- ‚úÖ **4 backups rotatifs automatiques** : Le script conserve automatiquement les 4 derni√®res configurations r√©ussies (`.last-success.{0,1,2,3}`)
- ‚úÖ **Sans `--backup`** : Rotation automatique, `.0` = plus r√©cent, `.3` = plus ancien
- ‚úÖ **Avec `--backup`** : Cr√©e un backup permanent timestamp√© (conserv√© 90 jours) + rotation automatique

**Exemple** :
```bash
# Premi√®re ex√©cution
sudo ./kubelet_auto_config.sh --profile gke
# Cr√©e : config.yaml.last-success.0

# Deuxi√®me ex√©cution
sudo ./kubelet_auto_config.sh --profile conservative
# Rotation : .0 ‚Üí .1
# Cr√©e : config.yaml.last-success.0 (nouveau)

# Rollback vers la config pr√©c√©dente
sudo cp /var/lib/kubelet/config.yaml.last-success.1 /var/lib/kubelet/config.yaml
sudo systemctl restart kubelet
```

### Q3 : Que se passe-t-il si mes pods d√©passent l'allocatable apr√®s modification ?

**R** : Kubernetes **n'√©vincera PAS** les pods d√©j√† running. Seuls les nouveaux pods seront soumis aux nouvelles limites. Pour forcer un r√©ajustement :

```bash
# Drainer le n≈ìud (optionnel)
kubectl drain <node-name> --ignore-daemonsets --delete-emptydir-data

# Appliquer la config
sudo ./kubelet_auto_config.sh --profile conservative --target-pods 110 --backup

# Rendre le n≈ìud schedulable
kubectl uncordon <node-name>
```

### Q4 : Comment choisir entre GKE et Conservative ?

| Crit√®re | GKE | Conservative |
|---------|-----|--------------|
| **Environnement** | Prod g√©n√©raliste | Critique (finance, sant√©) |
| **Densit√©** | < 80 pods/n≈ìud | > 80 pods/n≈ìud ou haute criticit√© |
| **Allocatable** | Maximis√© | R√©duit de ~15% |
| **Stabilit√©** | Excellente | Maximale |
| **Recommandation** | ‚úÖ D√©faut | Workloads sensibles |

### Q5 : Le script fonctionne-t-il avec cgroups v1 ?

**R** : Oui, le script est compatible cgroups v1 et v2. Il d√©tecte automatiquement la version via `systemd`.

### Q6 : Puis-je personnaliser les valeurs calcul√©es ?

**R** : Le script applique des formules √©prouv√©es. Pour des ajustements fins :

1. Utiliser `--dry-run` pour voir les valeurs
2. Modifier manuellement `/var/lib/kubelet/config.yaml` apr√®s ex√©cution
3. Ou √©diter le script (section `calculate_XXX()`)

### Q7 : Comment surveiller l'impact des r√©servations ?

**R** : M√©triques Prometheus √† surveiller :

```promql
# CPU throttling kubelet (doit √™tre < 5%)
rate(container_cpu_cfs_throttled_seconds_total{container="kubelet"}[5m])

# M√©moire disponible n≈ìud (doit √™tre > 1GiB)
node_memory_MemAvailable_bytes / 1024 / 1024 / 1024

# PLEG latency (doit √™tre < 5s)
kubelet_pleg_relist_duration_seconds{quantile="0.99"}
```

---

## üêõ Troubleshooting

### Probl√®me 0 : Le script affiche `!/bin/bash` au lieu du message d'aide

**Sympt√¥mes** :
```bash
./kubelet_auto_config.sh --help
# !/bin/bash
# ################################################################################
# Script de configuration automatique...
# (Le shebang #! est affich√© sans le #)
```

**Cause** : BOM UTF-8 (Byte Order Mark) invisible au d√©but du fichier

**D√©tection** :
```bash
# V√©rifier les 3 premiers octets
hexdump -C kubelet_auto_config.sh | head -1
# Si vous voyez "ef bb bf" ‚Üí BOM d√©tect√©

# Ou avec file
file kubelet_auto_config.sh
# Si vous voyez "UTF-8 Unicode (with BOM)" ‚Üí BOM d√©tect√©
```

**Solution automatique** :
```bash
# Utiliser le script de diagnostic fourni
bash debug_bom.sh

# Ou manuellement :
# 1. Backup
cp kubelet_auto_config.sh kubelet_auto_config.sh.backup

# 2. Supprimer les 3 premiers octets (BOM)
tail -c +4 kubelet_auto_config.sh > kubelet_auto_config.sh.tmp
mv kubelet_auto_config.sh.tmp kubelet_auto_config.sh
chmod +x kubelet_auto_config.sh

# 3. V√©rifier
./kubelet_auto_config.sh --help  # Doit afficher l'aide correctement
```

**Pr√©vention** :
- Le hook pre-commit Git d√©tecte automatiquement les BOM
- √âviter d'√©diter le script avec des √©diteurs Windows (Notepad)
- Utiliser `vim`, `nano`, ou VSCode avec encoding UTF-8 sans BOM

---

### Probl√®me 1 : Kubelet ne d√©marre pas apr√®s application

**Sympt√¥mes** :
```bash
sudo systemctl status kubelet
# ‚óè kubelet.service - kubelet: The Kubernetes Node Agent
#    Active: failed (Result: exit-code)
```

**Solution** :
```bash
# V√©rifier les logs
sudo journalctl -u kubelet -n 100 --no-pager

# Erreurs fr√©quentes :
# 1. "failed to parse kubelet config file"
#    ‚Üí Syntaxe YAML invalide dans /var/lib/kubelet/config.yaml
#    ‚Üí Restaurer le backup

# 2. "failed to create cgroup"
#    ‚Üí V√©rifier que systemd est bien le cgroup driver
#    ‚Üí V√©rifier : cat /var/lib/kubelet/config.yaml | grep cgroupDriver

# 3. "reservations exceed node capacity"
#    ‚Üí Les r√©servations sont trop √©lev√©es
#    ‚Üí Utiliser --profile minimal ou r√©duire --density-factor
```

### Probl√®me 2 : Allocatable trop faible, pas assez de place pour les pods

**Sympt√¥mes** :
```bash
kubectl describe node <node>
# Allocatable:
#   cpu: 100m  # Presque rien !
#   memory: 1Gi
```

**Cause** : R√©servations trop √©lev√©es (profil conservative + density-factor trop grand)

**Solution** :
```bash
# 1. V√©rifier les r√©servations actuelles
sudo cat /var/lib/kubelet/config.yaml | grep -A 3 "Reserved:"

# 2. Reconfigurer avec un profil moins conservateur
sudo ./kubelet_auto_config.sh --profile gke --backup

# 3. Ou r√©duire le density-factor
sudo ./kubelet_auto_config.sh --profile conservative --density-factor 1.2 --backup
```

### Probl√®me 3 : N≈ìud devient NotReady apr√®s configuration

**Sympt√¥mes** :
```bash
kubectl get nodes
# NAME    STATUS     ROLES    AGE   VERSION
# node1   NotReady   <none>   10d   v1.32.0
```

**Diagnostic** :
```bash
# 1. V√©rifier le kubelet
ssh node1 "sudo systemctl status kubelet"

# 2. V√©rifier les conditions du n≈ìud
kubectl describe node node1 | grep -A 20 "Conditions:"

# Causes fr√©quentes :
# - MemoryPressure: True    ‚Üí system-reserved m√©moire trop faible
# - DiskPressure: True      ‚Üí ephemeral-storage mal configur√©
# - NetworkUnavailable      ‚Üí Probl√®me CNI (non li√© au script)
```

**Solution** :
```bash
# Augmenter les r√©servations
ssh node1 "sudo /usr/local/bin/kubelet_auto_config.sh \
  --profile conservative \
  --density-factor 1.5 \
  --backup"
```

### Probl√®me 4 : √âvictions fr√©quentes apr√®s d√©ploiement

**Sympt√¥mes** :
```bash
kubectl get events --field-selector reason=Evicted
# REASON    MESSAGE
# Evicted   The node was low on resource: memory
```

**Diagnostic** :
```bash
# V√©rifier la pression m√©moire sur les n≈ìuds affect√©s
ssh node1 "cat /sys/fs/cgroup/kubepods.slice/memory.pressure"

# V√©rifier la m√©moire disponible
ssh node1 "free -h"
```

**Solution** :
```bash
# Augmenter system-reserved et kube-reserved
sudo ./kubelet_auto_config.sh \
  --profile conservative \
  --density-factor 1.5 \
  --backup

# OU ajuster manuellement les seuils d'√©viction
sudo vi /var/lib/kubelet/config.yaml
# Modifier :
# evictionHard:
#   memory.available: "1Gi"  # Au lieu de 500Mi
```

### Probl√®me 5 : Script √©choue avec "command not found"

**Sympt√¥mes** :
```bash
./kubelet_auto_config.sh
# line 42: bc: command not found
```

**‚ö†Ô∏è Note** : Depuis la **v2.0.16**, le script installe **automatiquement** les d√©pendances manquantes (`bc`, `jq`, `yq v4`). Ce probl√®me ne devrait plus se produire.

**Si le probl√®me persiste** :

1. **V√©rifier les permissions** : Le script a besoin de `sudo` pour installer les d√©pendances
   ```bash
   sudo ./kubelet_auto_config.sh
   ```

2. **V√©rifier la connectivit√© Internet** : Le script t√©l√©charge `yq` depuis GitHub
   ```bash
   ping -c 3 github.com
   ```

3. **Installation manuelle** (si n√©cessaire) :
   ```bash
   sudo apt update && sudo apt install -y bc jq systemd

   # Pour yq v4
   ARCH=$(uname -m)
   case "$ARCH" in
     x86_64|amd64)   YQ_BIN=yq_linux_amd64 ;;
     arm64|aarch64)  YQ_BIN=yq_linux_arm64 ;;
   esac
   sudo wget -qO /usr/local/bin/yq \
     "https://github.com/mikefarah/yq/releases/download/v4.44.3/${YQ_BIN}"
   sudo chmod +x /usr/local/bin/yq

   # V√©rifier
   which bc jq yq systemctl
   ```

### Probl√®me 6 : Permission denied lors de l'ex√©cution

**Sympt√¥mes** :
```bash
./kubelet_auto_config.sh
# bash: ./kubelet_auto_config.sh: Permission denied
```

**Solution** :
```bash
# Rendre le script ex√©cutable
chmod +x kubelet_auto_config.sh

# Ex√©cuter avec sudo
sudo ./kubelet_auto_config.sh
```

### Probl√®me 7 : Les cgroups ne sont pas cr√©√©s

**Sympt√¥mes** :
```bash
systemd-cgls | grep kubelet.slice
# (aucun r√©sultat)
```

**Diagnostic** :
```bash
# V√©rifier la configuration kubelet
sudo cat /var/lib/kubelet/config.yaml | grep -E "(enforceNodeAllocatable|Cgroup)"

# V√©rifier les logs
sudo journalctl -u kubelet | grep -i cgroup
```

**Solution** :
```bash
# S'assurer que enforceNodeAllocatable contient bien :
# - "pods"
# - "system-reserved"
# - "kube-reserved"

# R√©appliquer la configuration
sudo ./kubelet_auto_config.sh --profile conservative --backup
```

### Probl√®me 8 : Valeurs diff√©rentes entre n≈ìuds du m√™me type

**Sympt√¥mes** :
```bash
# node1 : Allocatable 15480m CPU
# node2 : Allocatable 15200m CPU (m√™me config mat√©rielle)
```

**Cause** : Le script a √©t√© ex√©cut√© avec des param√®tres diff√©rents

**Solution** :
```bash
# Standardiser sur tous les n≈ìuds
ansible all -i inventory.ini -m shell -a \
  "/usr/local/bin/kubelet_auto_config.sh \
  --profile conservative \
  --target-pods 110 \
  --backup"
```

---

## üìä Monitoring et m√©triques

### Lab de monitoring complet

Un environnement de monitoring complet (Prometheus + Grafana + Alerting) est disponible dans **`tests/kubelet-alerting-lab/`**.

**Contenu du lab** :
- üéØ Dashboard Grafana pr√™t √† l'emploi (JSON)
- üìà Recording rules Prometheus (m√©triques custom)
- üö® Alerting rules (5 alertes recommand√©es)
- üìñ Guide de d√©ploiement complet avec kube-prometheus-stack

**D√©ploiement rapide** :
```bash
cd tests/kubelet-alerting-lab
# Suivre le README.md pour d√©ployer sur votre cluster
```

**Ce qui est d√©ploy√©** :
- Helm chart `kube-prometheus-stack` (Prometheus + Grafana + Alertmanager)
- Metrics Server (m√©triques CPU/Memory des n≈ìuds)
- PrometheusRules (recording + alerting)
- Dashboard Grafana avec visualisation temps r√©el

---

### Recording rules Prometheus

**Fichier** : `tests/kubelet-alerting-lab/kubelet-reservations-recordings.yaml`

‚ö†Ô∏è **Pr√©requis** : N√©cessite **kube-prometheus-stack** ou **Prometheus Operator** (PrometheusRule CRD)

Ces recording rules cr√©ent des **m√©triques custom** bas√©es sur vos r√©servations kubelet :

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: kubelet-reservations-recordings
  namespace: monitoring
spec:
  groups:
    - name: kubelet-reservations
      rules:
        # M√©triques CPU r√©serv√©es (en cores)
        - record: kubelet_system_reserved_cpu_cores
        - record: kubelet_kube_reserved_cpu_cores

        # M√©triques m√©moire r√©serv√©es (en bytes)
        - record: kubelet_system_reserved_memory_bytes
        - record: kubelet_kube_reserved_memory_bytes
        - record: kubelet_memory_eviction_bytes

        # M√©triques en pourcentage de la capacit√©
        - record: kubelet_system_reserved_cpu_percent
        - record: kubelet_kube_reserved_cpu_percent
        - record: kubelet_system_reserved_memory_percent
        - record: kubelet_kube_reserved_memory_percent
        - record: kubelet_memory_eviction_percent
```

üìù **Important** : Les valeurs dans le fichier YAML sont des exemples. Adaptez-les selon votre configuration r√©elle (voir le README du lab pour g√©n√©rer automatiquement les bonnes valeurs).

**D√©ploiement** :
```bash
kubectl apply -f tests/kubelet-alerting-lab/kubelet-reservations-recordings.yaml
```

---

### Dashboard Grafana

**Fichier** : `tests/kubelet-alerting-lab/grafana-dashboard-kubelet-reservations.json`

Le dashboard affiche en temps r√©el les m√©triques de r√©servations kubelet :

**Panneaux inclus** :
- CPU system-reserved et kube-reserved (en cores)
- M√©moire system-reserved, kube-reserved et √©viction (en Mi)
- Allocatable CPU et m√©moire (capacit√© disponible pour les pods)
- Pourcentages de r√©servations par rapport √† la capacit√©
- Graphiques empil√©s pour visualiser la r√©partition

**Import du dashboard** :
1. Acc√©der √† Grafana ‚Üí **Dashboards** ‚Üí **Import**
2. Copier-coller le contenu du fichier JSON
3. S√©lectionner la datasource **Prometheus**
4. Cliquer sur **Import**

Le dashboard utilise les m√©triques cr√©√©es par les recording rules ci-dessus.

---

### Alertes recommand√©es

**Fichier** : `tests/kubelet-alerting-lab/kubelet-reservations-alerts.yaml`

‚ö†Ô∏è **Pr√©requis** : N√©cessite **kube-prometheus-stack** ou **Prometheus Operator** (PrometheusRule CRD)

**D√©ploiement** :
```bash
kubectl apply -f tests/kubelet-alerting-lab/kubelet-reservations-alerts.yaml
```

**5 alertes impl√©ment√©es** :

| Alerte | Condition | Dur√©e | S√©v√©rit√© | Action recommand√©e |
|--------|-----------|-------|----------|-------------------|
| **KubeletHighCPUThrottling** | Throttling CPU > 10% | 10 min | Warning | Augmenter kube-reserved CPU |
| **KubeletPLEGHighLatency** | PLEG P99 > 5s | 5 min | Warning | Augmenter kube-reserved ou r√©duire densit√© pods |
| **KubeletHighMemoryUsage** | RSS kubelet > 4 GiB | 10 min | Warning | Augmenter kube-reserved memory |
| **FrequentPodEvictions** | > 5 √©victions/min | 5 min | Critical | V√©rifier system-reserved et kube-reserved |
| **NodeLowAllocatable** | Allocatable < 80% capacity | 30 min | Warning | R√©servations potentiellement trop √©lev√©es |

**D√©tails des alertes** :

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: kubelet-reservations
  namespace: monitoring
spec:
  groups:
    - name: kubelet-reservations
      interval: 30s
      rules:

        # Alerte : Kubelet CPU throttling √©lev√©
        - alert: KubeletHighCPUThrottling
          expr: |
            rate(container_cpu_cfs_throttled_seconds_total{container="kubelet"}[5m]) > 0.1
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "Kubelet throttling CPU √©lev√© sur {{ $labels.node }}"
            description: "Le kubelet sur {{ $labels.node }} subit un throttling CPU de {{ $value | humanizePercentage }}. Augmentez kube-reserved CPU."

        # Alerte : PLEG latency trop √©lev√©e
        - alert: KubeletPLEGHighLatency
          expr: |
            histogram_quantile(0.99, rate(kubelet_pleg_relist_duration_seconds_bucket[5m])) > 5
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "PLEG latency √©lev√©e sur {{ $labels.node }}"
            description: "P99 PLEG latency = {{ $value }}s (seuil: 5s). Consid√©rez augmenter kube-reserved ou r√©duire la densit√© de pods."

        # Alerte : M√©moire kubelet √©lev√©e
        - alert: KubeletHighMemoryUsage
          expr: |
            process_resident_memory_bytes{job="kubelet"} / 1024 / 1024 / 1024 > 4
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "Consommation m√©moire kubelet √©lev√©e sur {{ $labels.instance }}"
            description: "Kubelet utilise {{ $value | humanize }}GiB de RAM. Augmentez kube-reserved memory."

        # Alerte : √âvictions fr√©quentes
        - alert: FrequentPodEvictions
          expr: |
            rate(kube_pod_status_reason{reason="Evicted"}[15m]) * 60 > 5
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "√âvictions fr√©quentes d√©tect√©es"
            description: "{{ $value | humanize }} √©victions/min. V√©rifiez les r√©servations system-reserved et kube-reserved."

        # Alerte : Allocatable tr√®s faible
        - alert: NodeLowAllocatable
          expr: |
            (kube_node_status_allocatable{resource="cpu"} / kube_node_status_capacity{resource="cpu"}) < 0.8
          for: 30m
          labels:
            severity: warning
          annotations:
            summary: "Allocatable CPU faible sur {{ $labels.node }}"
            description: "Seulement {{ $value | humanizePercentage }} de CPU allocatable. R√©servations potentiellement trop √©lev√©es."
```

**V√©rification des alertes** :
```bash
# V√©rifier que les PrometheusRules sont charg√©es
kubectl get prometheusrule -n monitoring

# V√©rifier les alertes actives dans Prometheus
# Acc√©der √† Prometheus UI ‚Üí Alerts
```

---

## üîê S√©curit√© et bonnes pratiques

### 1. Permissions du script

```bash
# Le script doit appartenir √† root et ne pas √™tre modifiable par d'autres
sudo chown root:root kubelet_auto_config.sh
sudo chmod 750 kubelet_auto_config.sh

# V√©rifier
ls -l kubelet_auto_config.sh
# -rwxr-x--- 1 root root 28472 Jan 20 10:30 kubelet_auto_config.sh
```

### 2. Audit trail

```bash
# Toutes les modifications sont loggu√©es dans syslog
sudo grep "configure-kubelet-reservations" /var/log/syslog

# Ou journalctl
sudo journalctl -t configure-kubelet-reservations
```

### 3. Validation avant production

**Checklist** :
- [ ] Test√© en dry-run sur 1 n≈ìud de dev
- [ ] Test√© en r√©el sur 1 n≈ìud de dev pendant 24h
- [ ] V√©rifi√© allocatable, pas d'√©victions
- [ ] Test√© workload r√©el (charge progressive)
- [ ] Valid√© par l'√©quipe infra/ops
- [ ] Documentation mise √† jour
- [ ] Proc√©dure de rollback test√©e

### 4. Versioning du script

```bash
# Ajouter un num√©ro de version dans le script
VERSION="1.0.0"

# Commit dans Git
git add kubelet_auto_config.sh
git commit -m "feat: script configuration kubelet v1.0.0"
git tag v1.0.0
git push origin v1.0.0
```

---

## üìö Ressources suppl√©mentaires

### Documentation officielle Kubernetes

- [Reserve Compute Resources](https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/)
- [Node Allocatable Resources](https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable)
- [Kubelet Configuration Reference](https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/)

### Documentation cloud providers

- [GKE Node Allocatable](https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-architecture#node_allocatable_resources)
- [EKS Allocatable Capacity](https://docs.aws.amazon.com/eks/latest/userguide/allocatable-capacity.html)
- [AKS Resource Reservations](https://learn.microsoft.com/en-us/azure/aks/concepts-clusters-workloads#resource-reservations)

### Benchmarks et √©tudes

- [Kubernetes SIG Scalability Thresholds](https://github.com/kubernetes/community/blob/master/sig-scalability/configs-and-limits/thresholds.md)
- [CNCF Cloud Native Landscape](https://landscape.cncf.io/)

---

## ü§ù Contribution

### Signaler un bug

Ouvrez une issue sur GitHub avec :
- Version du script (voir `--help`)
- Version Kubernetes (`kubectl version`)
- OS et version (`cat /etc/os-release`)
- Logs complets (`journalctl -u kubelet`)

### Proposer une am√©lioration

1. Fork le repository
2. Cr√©ez une branche (`git checkout -b feature/nouvelle-fonctionnalite`)
3. Committez (`git commit -am 'feat: ajout fonctionnalit√© X'`)
4. Push (`git push origin feature/nouvelle-fonctionnalite`)
5. Ouvrez une Pull Request

---


## üìù Changelog et Notes de Version

Pour l'historique complet des versions, consultez les fichiers de changelog d√©di√©s :

- **[CHANGELOG_v2.0.16.md](CHANGELOG_v2.0.16.md)** - Version actuelle (installation automatique des d√©pendances)
- **[CHANGELOG_v2.0.15.md](CHANGELOG_v2.0.15.md)** - Lab monitoring kubelet (Prometheus/Grafana)
- **[CHANGELOG_v2.0.14.md](CHANGELOG_v2.0.14.md)** - Validation compl√®te des 3 m√©thodes de d√©ploiement
- **[CHANGELOG_v2.0.13.md](CHANGELOG_v2.0.13.md)** - Garde-fous allocatable & diff automatiques
- **[CHANGELOG_v2.0.12.md](CHANGELOG_v2.0.12.md)** - R√©servations √©ph√©m√®res adaptatives & robustesse kubelet
- **[CHANGELOG_v2.0.11.md](CHANGELOG_v2.0.11.md)** - D√©tection automatique control-plane/worker
- **[CHANGELOG_v2.0.10.md](CHANGELOG_v2.0.10.md)** - Correctifs tests critiques
- **[CHANGELOG_v2.0.9.md](CHANGELOG_v2.0.9.md)** - Am√©lioration UX suite de tests
- **[CHANGELOG_v2.0.8.md](CHANGELOG_v2.0.8.md)** - Correctifs critiques ARM64
- Versions pr√©c√©dentes : voir le dossier `changelogs/` (si cr√©√©)

### Version Actuelle : v2.0.16

**Nouveaut√©s v2.0.16 (Installation automatique des d√©pendances) :**
- ‚úÖ **Installation automatique** : Le script installe automatiquement `bc`, `jq`, et `yq v4` si manquants
- ‚úÖ **D√©tection d'architecture** : Support ARM64 et AMD64 automatique pour yq
- ‚úÖ **Remplacement automatique** : Remplace yq Python v3 par yq v4 (mikefarah) si d√©tect√©
- ‚úÖ **Zero-config** : Une seule commande suffit, aucune pr√©paration manuelle
- ‚úÖ **Gain de temps** : 5-10 minutes √©conomis√©es par installation
- ‚úÖ **Coh√©rence** : M√™me logique que le playbook Ansible

**Versions pr√©c√©dentes notables :**
- v2.0.15 : Lab monitoring kubelet (Prometheus/Grafana, alertes, dashboard)
- v2.0.14 : Validation des 3 m√©thodes de d√©ploiement (Manuel, Ansible, DaemonSet)
- v2.0.13 : Garde-fous allocatable, diff automatiques, r√©servations √©ph√©m√®res

**Script version v2.0.13 (inclus) :**
- ‚úÖ Garde-fous : density-factor plafonn√© sur control-planes, arr√™t si allocatable < 25% CPU / 20% RAM
- ‚úÖ Affichage de la variation estim√©e et r√©elle d'allocatable
- ‚úÖ R√©servations `ephemeral-storage` dynamiques selon capacit√© n≈ìud
- ‚úÖ D√©tection automatique control-plane vs worker avec enforcement adapt√©
- ‚úÖ Support ARM64 complet & suite de tests unitaires (38/38)
- ‚úÖ Compatible `set -euo pipefail`

Voir [CHANGELOG_v2.0.16.md](CHANGELOG_v2.0.16.md) pour les d√©tails complets de la version actuelle.

---
## üìÑ Licence

MIT License

Copyright (c) 2025

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

---

## üÜò Support

### Communaut√©

- **GitLab Issues** : https://gitlab.com/omega8280051/reserved-sys-kube/-/issues

---

## ‚ú® Cr√©dits

**D√©velopp√© par** : un stagiaire nomm√© Claude. Mais avec un Senior derri√®re lui quand m√™me. 

**Bas√© sur** :
- Formules officielles Google (GKE)
- Formules officielles Amazon (EKS)
- Recommandations Red Hat (OpenShift)
- Benchmarks Kubernetes SIG Scalability

**Remerciements** :
- Communaut√© Kubernetes
- CNCF (Cloud Native Computing Foundation)
- Contributeurs open source

---

**Note** : Ce script a √©t√© test√© sur les distributions suivantes :
- ‚úÖ Ubuntu 20.04, 22.04, 24.04

**Versions Kubernetes test√©es** :
- ‚úÖ v1.26.x
- ‚úÖ v1.27.x
- ‚úÖ v1.28.x
- ‚úÖ v1.29.x
- ‚úÖ v1.30.x
- ‚úÖ v1.31.x
- ‚úÖ v1.32.x

---

**Derni√®re mise √† jour** : 22 oct 2025
**Version du projet** : 2.0.16 (script v2.0.13)
**Mainteneur** : Platform Engineering Team
